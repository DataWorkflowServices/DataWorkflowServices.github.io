{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DataWorkflowServices","text":"<p>DataWorkflowServices, also known as DWS, is HPC batch software that integrates with the DWS API and HPC systems and storage systems to provide intelligent data movement and ephemeral storage resources to user workloads.</p> <p>Here you will find DWS User Guides and Examples.</p> <p>Documention covering management of the repo are in Repo Guides.</p>"},{"location":"guides/","title":"User Guides","text":""},{"location":"guides/#setup","title":"Setup","text":"<ul> <li>DWS on CSM</li> </ul>"},{"location":"guides/#operation","title":"Operation","text":"<ul> <li>Workflow Errors</li> </ul>"},{"location":"guides/dws-on-csm/readme/","title":"DWS On CSM: Installation and use of DWS on a CSM cluster","text":"<p>DataWorkflowServices (DWS) on CSM consists of two parts.  The first part is the DWS service itself, which must be installed on the CSM cluster.  The second part is the Slurm Burst Buffer Plugin which must be installed on the system where Slurm is running. The Burst Buffer Plugin is used by Slurm to talk to the DWS API.</p>"},{"location":"guides/dws-on-csm/readme/#prepare-for-install-or-upgrade-of-dws","title":"Prepare For Install Or Upgrade of DWS","text":"<p>If DWS is not yet installed on the CSM cluster, then proceed to the Install DWS section.</p> <p>If DWS is already installed on the CSM cluster then that version must be undeployed before the next version can be installed.  This is necessary to properly handle updates to the DWS Custom Resource Definitions (CRDs).  Proceed to the Undeploy DWS section before returning here to install the new version.</p>"},{"location":"guides/dws-on-csm/readme/#install-dws","title":"Install DWS","text":""},{"location":"guides/dws-on-csm/readme/#retrieve-dws-configuration-and-container-image","title":"Retrieve DWS Configuration and Container Image","text":"<p>The DWS Configuration is in the DWS repo.  It is not necessary to build DWS, but this is where its configuration files will be found.  These include the DWS CRDs, Deployment, ServiceAccount, Roles and bindings, Services, and Secrets.</p> ncn-m001:~ #<pre><code>DWS_VER=0.0.20\ngit clone --branch v$DWS_VER https://github.com/DataWorkflowServices/dws.git dws-$DWS_VER\ncd dws-$DWS_VER\n</code></pre> <p>This workarea must be preserved.  It will be used again when this version of DWS must be undeployed.</p>"},{"location":"guides/dws-on-csm/readme/#load-the-dws-container-images","title":"Load the DWS Container Images","text":"<p>DWS uses two container images which must be loaded into the CSM cluster's container registry. The Nexus Admin Credential will be used to push these images into the registry.</p> <p>Obtain the Nexus Admin user name.  Store it in a variable to be used in later commandlines:</p> ncn-m001:~/dws #<pre><code>NEXUS_USER=$(kubectl get secret -n nexus nexus-admin-credential -o json | jq .data.username -r | base64 -d)\n</code></pre> <p>Obtain the Nexus Admin password.  Keep this output available so it can be used with cut-and-paste when podman requests the password:</p> ncn-m001:~/dws #<pre><code>kubectl get secret -n nexus nexus-admin-credential -o json | jq .data.password -r | base64 -d\n</code></pre> <p>Use the Nexus Admin user name and password to load the container images into the container registry.  Adjust or replace the following <code>podman pull</code> commands as necessary, depending on your network restrictions, to copy each container in from <code>ghcr.io</code>.</p> <p>Get the DWS container image corresponding to this repo:</p> ncn-m001:~/dws #<pre><code>podman pull ghcr.io/dataworkflowservices/dws:$DWS_VER\npodman tag ghcr.io/dataworkflowservices/dws:$DWS_VER registry.local/dws:$DWS_VER\npodman push --creds $NEXUS_USER registry.local/dws:$DWS_VER\n</code></pre> <p>Get the kube-rbac-proxy container:</p> ncn-m001:~ #<pre><code>RBAC_PROXY_VER=v0.14.1\npodman pull docker://gcr.io/kubebuilder/kube-rbac-proxy:$RBAC_PROXY_VER\npodman tag gcr.io/kubebuilder/kube-rbac-proxy:$RBAC_PROXY_VER registry.local/kube-rbac-proxy:$RBAC_PROXY_VER\npodman push --creds $NEXUS_USER registry.local/kube-rbac-proxy:$RBAC_PROXY_VER\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#deploy-dws-to-the-cluster","title":"Deploy DWS to the cluster","text":"<p>Deploy DWS to the cluster:</p> ncn-m001:~/dws #<pre><code>make kustomize\nmake deploy OVERLAY=csm\n</code></pre> <p>Wait for the deployment and webhook to become ready.</p> ncn-m001:~/dws #<pre><code>kubectl wait deployment --timeout=120s -n dws-system dws-controller-manager --for condition=Available=True\nkubectl wait deployment --timeout=120s -n dws-system dws-webhook --for condition=Available=True\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#undeploy-dws","title":"Undeploy DWS","text":"<p>All Slurm jobs must be completed before DWS can be undeployed.  In addition, all DWS Workflow resources must have been deleted.</p> <p>Use the same workarea that was used to install DWS.  It should still be set to the same version of DWS that will be undeployed.</p> <p>Confirm that all DWS Workflow resources have been deleted:</p> ncn-m001:~/dws #<pre><code>kubectl get workflows.dataworkflowservices.github.io -A\nNo resources found\n</code></pre> <p>If any workflows remain, then some Slurm jobs are not yet completed.  All Slurm jobs must be completed before DWS can be undeployed.</p> <p>To undeploy DWS:</p> ncn-m001:~/dws #<pre><code>make undeploy OVERLAY=csm\n</code></pre> <p>Ignore any errors about not finding secrets.  They were removed by garbage collection during earlier steps in the processing of <code>kubectl delete</code>.</p>"},{"location":"guides/dws-on-csm/readme/#rbac-role-based-access-control","title":"RBAC: Role-Based Access Control","text":"<p>RBAC (Role Based Access Control) determines the operations a user or service can perform on a list of Kubernetes resources. RBAC affects everything that interacts with the kube-apiserver (both users and services internal or external to the cluster). More information about RBAC can be found in the Kubernetes documentation.</p> <p>This is a Slurm-specific version of the instructions found in RBAC: Role-Based Access Control for interacting with DWS. Consult that document for more details.</p>"},{"location":"guides/dws-on-csm/readme/#rbac-for-burst-buffer-plugin","title":"RBAC for Burst Buffer Plugin","text":"<p>A workload manager (WLM) such as Slurm will interact with DataWorkflowServices as a privileged user. RBAC is used to limit the operations that a WLM can perform:</p> <ul> <li>Generate a new key/cert pair for a \"slurm-dws\" user</li> <li>Creating a new kubeconfig file</li> <li>Adding RBAC rules for the \"slurm-dws\" user to allow appropriate access to the DataWorkflowServices API.</li> </ul> <p>Note Each of these steps must be performed on the CSM management node.</p>"},{"location":"guides/dws-on-csm/readme/#generate-a-key-and-certificate","title":"Generate a Key and Certificate","text":"<p>The first step is to create a new key and certificate for the \"slurm-dws\" user.  This will likely be done on one of the CSM management nodes. The <code>openssl</code> command needs access to the certificate authority file. This is typically located in <code>/etc/kubernetes/pki</code>.</p> ncn-m001:~/dws #<pre><code># make a temporary work space\nmkdir /tmp/slurm-dws\ncd /tmp/slurm-dws\n\n# Create this user\nexport USERNAME=slurm-dws\n\n# generate a new key\nopenssl genrsa -out slurm-dws.key 2048\n\n# create a certificate signing request for this user\nopenssl req -new -key slurm-dws.key -out slurm-dws.csr -subj \"/CN=$USERNAME\"\n\n# generate a certificate using the certificate authority on the k8s cluster. This certificate lasts 500 days\nopenssl x509 -req -in slurm-dws.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out slurm-dws.crt -days 500\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#create-a-kubeconfig","title":"Create a kubeconfig","text":"<p>After the keys have been generated, a new kubeconfig file can be created for this user. The admin kubeconfig <code>/etc/kubernetes/admin.conf</code> can be used to determine the cluster name kube-apiserver address.</p> ncn-m001:~/dws #<pre><code># get the cluster name and server\nCURRENT_CONTEXT=$(kubectl config current-context)\nCLUSTER_NAME=$(kubectl config get-contexts | grep $CURRENT_CONTEXT | awk '{print $3}')\nSERVER=$(kubectl config view -o jsonpath='{.clusters[?(@.name == \"'$CLUSTER_NAME'\")].cluster.server}')\n\n# create a new kubeconfig with the server information\nkubectl config set-cluster $CLUSTER_NAME --kubeconfig=slurm-dws.kubeconfig --server=$SERVER --certificate-authority=/etc/kubernetes/pki/ca.crt --embed-certs=true\n\n# add the key and cert for this user to the config\nkubectl config set-credentials $USERNAME --kubeconfig=slurm-dws.kubeconfig --client-certificate=slurm-dws.crt --client-key=slurm-dws.key --embed-certs=true\n\n# add a context\nkubectl config set-context $USERNAME --kubeconfig=slurm-dws.kubeconfig --cluster=$CLUSTER_NAME --user=$USERNAME\n\n# make it the default context\nkubectl config use-context $USERNAME --kubeconfig slurm-dws.kubeconfig\n</code></pre> <p>Important This new kubeconfig file must be installed as <code>/etc/slurm/slurm-dws.kubeconfig</code> on the system that has slurmctld and the burst buffer plugin.</p>"},{"location":"guides/dws-on-csm/readme/#apply-the-provided-clusterrole-and-create-a-clusterrolebinding","title":"Apply the provided ClusterRole and create a ClusterRoleBinding","text":"<p>DataWorkflowServices has already defined the role to be used with WLMs.  Simply apply the <code>workload-manager</code> ClusterRole from DataWorkflowServices to the system, found in the dws repo that was checked out earlier:</p> ncn-m001:~/dws #<pre><code>kubectl apply -f config/rbac/workload_manager_role.yaml\n</code></pre> <p>Create and apply a ClusterRoleBinding to associate the \"slurm-dws\" user with the <code>workload-manager</code> ClusterRole:</p> <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: slurm-dws-viewer\nsubjects:\n- kind: User\n  name: slurm-dws\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: workload-manager\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>That resource can be created using the <code>kubectl apply</code> command.</p>"},{"location":"guides/dws-on-csm/readme/#configure-slurm-for-dws","title":"Configure Slurm for DWS","text":"<p>Slurm provides an API for burst buffer plugins that it uses to talk to a workload manager (WLM).  The plugin is written in the Lua programming language and is placed in the same directory that holds the rest of the Slurm configuration files.  This plugin contains the logic required to communicate with DWS.</p> <p>This project will install two files into the slurm pod's <code>/etc/slurm</code> directory and will update the main Slurm configuration file to enable the plugin.  The first will be the Lua plugin script, named <code>burst_buffer.lua</code>, and the second will be a configuration file named <code>burst_buffer.conf</code> which tells Slurm that job scripts containing a <code>#DW</code> directive should be run through the burst_buffer plugin.</p> <p>The burst buffer plugin will use the <code>kubectl</code> command to access the Kubernetes environment where DWS is running, so the system running the slurmctld must have this command and a valid <code>kubeconfig</code> file installed as <code>/etc/slurm/slurm-dws.kubeconfig</code>. See RBAC: Role-Based Access Control above for the creation of this kubeconfig file.</p>"},{"location":"guides/dws-on-csm/readme/#checkout-the-dws-slurm-burst-buffer-plugin-repo","title":"Checkout the DWS Slurm Burst Buffer Plugin repo","text":"<p>On the Slurm system running slurmctl, check out the repo containing the burst buffer plugin and its configuration file:</p> <pre><code>BBPLUGIN_VER=0.0.5\ngit clone --branch v$BBPLUGIN_VER https://github.com/DataWorkflowServices/dws-slurm-bb-plugin.git dws-slurm-bb-plugin-$BBPLUGIN_VER\ncd dws-slurm-bb-plugin-$BBPLUGIN_VER\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#install-the-burst-buffer-plugin","title":"Install the burst buffer plugin","text":"<p>Copy the burst buffer plugin and its configuration file into the Slurm configuration directory from the repo you've checked out above:</p> ~/dws-slurm-bb-plugin #<pre><code>cp src/burst_buffer/burst_buffer.lua /etc/slurm\ncp src/burst_buffer/burst_buffer.conf /etc/slurm\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#enable-the-burst-buffer-plugin","title":"Enable the burst buffer plugin","text":"<p>On the system running slurmctld, edit <code>/etc/slurm/slurm.conf</code> to enable the use of the burst buffer plugin.</p> ~/dws-slurm-bb-plugin #<pre><code>echo \"BurstBufferType=burst_buffer/lua\" &gt;&gt; /etc/slurm/slurm.conf\n</code></pre> <p>Then restart slurmctld to pick up the new configuration.</p>"},{"location":"guides/dws-on-csm/readme/#prepare-to-submit-a-test-job","title":"Prepare to submit a test job","text":"<p>From the Slurm system that has slurmctld, where the burst buffer plugin will run, verify that the <code>/etc/slurm/slurm-dws.kubeconfig</code> file that was created earlier is available by verifying that it can be used to reach the DWS API:</p> <pre><code>kubectl --kubeconfig /etc/slurm/slurm-dws.kubeconfig get workflows.dataworkflowservices.github.io -A\n</code></pre> <p>There should be no workflows and you should see only the following output:</p> <pre><code>No resources found\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#submit-a-test-job","title":"Submit a test job","text":"<p>Create the following example test job:</p> $ cat /tmp/dws-test<pre><code>#!/bin/sh                \n#SBATCH --time=1\n#DW\n/bin/hostname\nsrun -l /bin/hostname\nsrun -l /bin/pwd\n</code></pre> <p>Submit the test job:</p> <pre><code>sbatch /tmp/dws-test\n</code></pre> <p>Get the DWS Workflow via <code>kubectl</code>, running from the Slurm system that has slurmctld.  If this step fails then return to Prepare to submit a test job to verify the correct kubectl configuration.</p> <pre><code>kubectl --kubeconfig /etc/slurm/slurm-dws.kubeconfig get workflows.dataworkflowservices.github.io -A\n</code></pre> <p>Get the status of the DWS Workflow via <code>scontrol</code>.  This will cause Slurm to use the burst buffer plugin to access the DWS API, and will use <code>kubectl</code> under the covers:</p> <pre><code>scontrol show bbstat workflow 7773 &amp;&amp; echo\n</code></pre> <p>The output will appear as:</p> <pre><code>desiredState=DataIn currentState=DataIn status=Completed\n</code></pre> <p>Note that the workflow resource will no longer exist after Slurm has completed its teardown state for this job.</p>"},{"location":"guides/dws-on-csm/readme/#canceling-a-test-job","title":"Canceling a test job","text":"<p>Use of the <code>scancel</code> command in states prior to PostRun will cause Slurm to pass the <code>hurry</code> flag to the teardown function in the burst_buffer.lua script, and the teardown function will set the flag in the Workflow resource.  In the PostRun or DataOut states the burst_buffer.lua teardown function will not be passed the <code>hurry</code> flag because no work will be skipped.  In all states, the <code>scancel --hurry</code> command will cause Slurm to pass the <code>hurry</code> flag to the teardown function.</p> <p>The <code>scancel</code> command will cause states prior to PostRun to terminate immediately and proceed to Teardown state.  The use of <code>scancel --hurry</code> does not alter this behavior on these pre-PostRun states.  During PostRun or DataOut, the <code>scancel</code> command does not cause early termination and does not skip the DataOut state.  The use of <code>scancel --hurry</code> during PostRun or DataOut causes early termination, skipping DataOut in the case of PostRun, and proceeds to Teardown.</p> <p>Consult the Slurm Burst Buffer Guide for further details on the use of <code>scancel</code> versus <code>scancel --hurry</code>.</p>"},{"location":"guides/dws-on-csm/readme/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/dws-on-csm/readme/#collect-slurmctld-logs","title":"Collect slurmctld logs","text":"<p>To pick out the burst buffer plugin messages from slurmctld logs, grep for the string \"lua\".</p>"},{"location":"guides/dws-on-csm/readme/#collect-dws-logs","title":"Collect DWS logs","text":"<p>The DWS logs can be retrieved with the following.  These commands should be run from the CSM management node.  Run the <code>kubectl</code> command using the adminstrator context:</p> ncn-m001:~ #<pre><code>DWS_POD=`kubectl --context kubernetes-admin@kubernetes get pods -n dws-system -l control-plane=controller-manager -o jsonpath='{.items[0].metadata.name}'`\nkubectl --context kubernetes-admin@kubernetes logs -n dws-system $DWS_POD -c manager\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#inspect-dws-workflow-resources","title":"Inspect DWS Workflow resources","text":"<p>Inspect DWS Workflow resources by using the fully-qualified CRD name.</p> ncn-m001:~ #<pre><code>kubectl --context kubernetes-admin@kubernetes get workflows.dataworkflowservices.github.io -A\n</code></pre> <p>The output will show the job ID used in the name of the Workflow resource:</p> <pre><code>NAMESPACE   NAME     STATE    READY   STATUS      AGE\nuser        bb7773   DataIn   true    Completed   13m\n</code></pre> <p>To get more detail about the workflow while it exists:</p> ncn-m001:~ #<pre><code>kubectl --context kubernetes-admin@kubernetes get workflow.dataworkflowservices.github.io -n user bb7773 -o yaml\n</code></pre> <p>Note that the Workflow resource will be deleted during the job's teardown state, at which time it will no longer appear in the workflow listing.</p> Multiple CRDs named \"Workflow\"     The fully-qualified name will protect you from picking up any resource named \"Workflow\" that belongs to some other service.  Your system may have multiple CRDs named \"Workflow\".  To see the CRDs named \"Workflow\":  ncn-m001:~ #<pre><code>kubectl --context kubernetes-admin@kubernetes get crds | grep -E '^workflows\\.'\n</code></pre>  Example output:  <pre><code>workflows.argoproj.io                                               2022-09-16T14:21:54Z\nworkflows.dataworkflowservices.github.io                            2023-09-28T19:20:23Z\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#references","title":"References","text":"<ul> <li>Slurm Burst Buffer Guide </li> <li>Slurm burst_buffer.conf manage</li> </ul>"},{"location":"guides/errors/readme/","title":"Workflow Errors","text":"<p>A workflow may fail to complete the desired state for many reasons. The best action for the WLM to take depends on what is preventing the workflow from progressing. DWS communicates error information back to the WLM through a few different fields so the WLM can take the best course of action.</p>"},{"location":"guides/errors/readme/#drivers-and-directives","title":"Drivers and Directives","text":"<p>Most of the work required to move a workflow into the desired state is performed by software outside of DWS. These external pieces of software are called drivers. An example of a driver is the NNF software.</p> <p>Directives are the #DW lines an end user provides as part of their job. There can be more than one directive in a job, and therefore more than one directive in a DWS workflow.</p> <p>Directives are processed individually by the underlying drivers. At workflow creation time, the drivers modify the workflow to indicate which workflow states they are responsible for completing. This occurs for each of the directives. For each workflow state, DWS then waits for all the drivers to complete their work for each of the directives before marking the entire workflow as complete.</p>"},{"location":"guides/errors/readme/#workflow-status","title":"Workflow Status","text":"<p>The current status of the workflow is reported in the <code>workflow.status.status</code> field. The following summarizes the values for this field:</p> <ol> <li><code>DriverWait</code> - The workflow is progressing towards completion, but it's waiting on one or more drivers to complete their work</li> <li><code>Completed</code> - All drivers have finished their work and the workflow has reached the state in <code>workflow.status.state</code></li> <li><code>TransientCondition</code> - One or more drivers have encountered an error that might resolve</li> <li><code>Error</code> - One or more drivers have encountered an error that will not recover</li> </ol> <p>If there are multiple directives in the workflow, the directive with the highest severity status is reported in the workflow.</p>"},{"location":"guides/errors/readme/#workflow-message","title":"Workflow Message","text":"<p>The <code>workflow.status.message</code> field provides a human readable message about the current progress of the workflow. When the <code>workflow.status.status</code> is <code>TransientCondition</code> or <code>Error</code>, the <code>message</code> field is an error message. This error message is meant to provide useful information about the error to end users.</p> <p>The error messages are prefixed with the index of the directive in the workflow that generated the error. For example <code>DW Directive 2</code>.</p> <p>These error messages are also prefixed with one of three labels:</p> <ol> <li><code>User error</code> - A driver was unable to progress due to a bad input from an admin or an end user</li> <li><code>WLM error</code> - A driver was unable to progress due to a bad input from the WLM</li> <li><code>Internal error</code> - A driver was unable to progress due to an unexpected software or hardware issue</li> </ol> <p>Below is an example of a full error message: <pre><code>DW Directive 0: User error: only a single create_persistent or destroy_persistent directive is allowed per workflow\n</code></pre></p> <p>If there are multiple directives that encountered an error of the same severity, only one of the error messages is reported back up to the <code>workflow.status.message</code> field.</p> <p>Depending on the type of error, the <code>workflow.status.message</code> field may not provide enough information for an admin to determine exactly what went wrong. This field is meant for end users, so the message purposefully hides driver implementation details.</p>"},{"location":"guides/errors/readme/#internal-error-message","title":"Internal Error Message","text":"<p>A more thorough debug message can be found in the <code>workflow.status.drivers[].message</code> field. The <code>workflow.status.drivers</code> array has an entry for each of the workflow states that a driver has registered for a directive. By using the directive index from the <code>workflow.status.message</code> field and the current workflow state, the correct array entry can be found. The <code>workflow.status.drivers[].dwdIndex</code> and <code>workflow.status.drivers[].watchState</code> fields contain the index and workflow state.</p> <p>The error message in the <code>workflow.status.drivers[].message</code> will contain more driver specific detail about the error. This message is not intended for end users, but it may be useful for a system admin. An example error message is as follows:</p> <pre><code>internal error: storage resource error: default/lustre-mgs-pool-0: unable to format file system for allocation 0: could not create file share: Error 500: Internal Server Error, Retry-Delay: 0s, Cause: File share 'default-lustre-mgs-pool-0-mgt-0-0' failed to create, Internal Error: Error 500: Internal Server Error, Retry-Delay: 0s, Resource: #FileShare.v1_2_0.FileShare, Cause: File share 'default-lustre-mgs-pool-0-mgt-0-0' create failed Internal Error: Error Running Command 'zpool create -O canmount=off -o cachefile=none zf2916ba-mgtpool-0 /dev/nvme12n1 /dev/nvme11n1 /dev/nvme10n1 /dev/nvme1n1 /dev/nvme4n1 /dev/nvme14n1 /dev/nvme16n1 /dev/nvme5n1 /dev/nvme8n1 /dev/nvme3n1 /dev/nvme2n1 /dev/nvme13n1 /dev/nvme15n1 /dev/nvme7n1 /dev/nvme6n1 /dev/nvme9n1', StdErr: cannot label 'nvme12n1': try using parted(8) and then provide a specific slice: -4\n</code></pre>"},{"location":"guides/errors/readme/#wlm-actions","title":"WLM Actions","text":"<p>When a workflow fails to progress due to an error, the WLM should change its response based on the type of error. When <code>workflow.status.status</code> is <code>Error</code>, the workflow will not recover from the error, and the WLM should move the workflow to state <code>Teardown</code> immediately. When <code>workflow.status.status</code> is <code>TransientCondition</code>, the driver code will continue to retry in the background, and the issue may resolve. The WLM should wait a short amount of time before moving the workflow to state <code>Teardown</code>. Thirty seconds is the recommended wait period.</p>"},{"location":"repo-guides/","title":"Repo Guides","text":""},{"location":"repo-guides/#management","title":"Management","text":"<ul> <li>Create a Release</li> </ul>"},{"location":"repo-guides/create-a-release/readme/","title":"Create a Release: Notes about creating a release","text":"<p>The following notes describe how to use the GitHub release process to create a release of a repo, and the workflows in place to guide this process.  This includes the use of release branches and annotated release tags.</p>"},{"location":"repo-guides/create-a-release/readme/#github-release-process","title":"GitHub Release Process","text":"<p>See About releases for the GitHub documentation on releases.</p> <p>A release can be created on GitHub by clicking the <code>Create a new release</code> button in the repo.  Select an annotated tag for your release. Do not use this interface to create a tag because that tag will not be annotated and it will fail the repo's workflow steps.  See the steps below to create an annotated tag, then proceed to the remaining steps in the GitHub documentation.</p>"},{"location":"repo-guides/create-a-release/readme/#release-branches","title":"Release branches","text":"<p>Release branches are named <code>releases/vX</code>, where X is 1, 2, etc.  In the usual case, all version 1.x.y release tags will go into the <code>releases/v1</code> branch, and all version 2.x.y tags will go into the <code>releases/v2</code> branch.  Fine-grained branches such as <code>releases/v2.1</code> for all version 2.1.y tags may be used if necessary.</p> <p>The repo has a \"branch protection rule\" on the branch pattern of <code>releases/v*</code> to enforce the use of pull requests and reviews on release branches.  See Managing a branch protection rule.</p>"},{"location":"repo-guides/create-a-release/readme/#release-tags","title":"Release tags","text":"<p>Release tags are named in patterns like <code>vX.Y.Z</code> or <code>vX.Y.Z-alpha</code> and are applied to the release branches.</p> <p>The repo has a \"tag protection rule\" on the tag pattern of <code>v*</code>.  Only users who have admin or maintain permission may create or delete protected tags.  See Configuring tag protection rules.</p> <p>IMPORTANT The tag must be applied to a commit that is entirely within the release branch, so that it is not visible from the main branch.  If it is visible on the main branch, then image tags from the main branch will collide with image tags on the release branch.  This means that after you create the release branch you may have to add another commit to it simply to have a place to attach the tag.</p> Verify tag visibility     After you've created the tag in your workarea, but before you've pushed it to the repo, verify that it is visible only on the release branch and not on the main branch.  Look for tags on the main branch<pre><code>git checkout main\ngit describe --match=\"v*\" HEAD\n</code></pre> Look for tags on the release branch<pre><code>git checkout releases/v1\ngit describe --match=\"v*\" HEAD\n</code></pre>"},{"location":"repo-guides/create-a-release/readme/#annotated-tags","title":"Annotated tags","text":"<p>Our process requires that release tags be annotated.  An annotated tag contains information about who created it and when it was created.  Annotated tags may also be signed.</p> <p>The repo contains a workflow which verifies that any pushed tag is an annotated tag.</p>"},{"location":"repo-guides/create-a-release/readme/#creating-an-annotated-tag","title":"Creating an annotated tag.","text":"<p>You can use GitHub Desktop to create an annotated tag, or you can create the tag with the <code>git tag -a</code> command.</p> Example annotated tag<pre><code>git checkout releases/v1\ngit tag -a v1.0.2 -m \"My release 1.0.2\"\ngit push origin --tags\n</code></pre> Verify that a tag is annotated When a tag is annotated, its details will be displayed before the commit the tag references.  Show a tag<pre><code>git show v0.0.6\n</code></pre>  If the tag is annotated then its details will be shown before the commit is shown.   An annotated tag<pre><code>tag v0.0.6\nTagger: Dean Roehrich &lt;dean.roehrich@hpe.com&gt;\nDate:   Wed Dec 14 15:15:46 2022 -0600\n\nExperimental release #8\n\ncommit 91fea7d02a5e7196ca8b4686deb33898109fd8c0 (HEAD, tag: v0.0.6)\nAuthor: Dean Roehrich &lt;dean.roehrich@hpe.com&gt;\nDate:   Wed Dec 14 14:44:15 2022 -0600\n\n    Rename the workflow that verifies tags. (#82)\n\n    Signed-off-by: Dean Roehrich &lt;dean.roehrich@hpe.com&gt;\n\n[...]\n</code></pre>"}]}