{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DataWorkflowServices","text":"<p>DataWorkflowServices, also known as DWS, is HPC batch software that integrates with the DWS API and HPC systems and storage systems to provide intelligent data movement and ephemeral storage resources to user workloads.</p> <p>Here you will find DWS User Guides and Examples.</p> <p>Documention covering management of the repo are in Repo Guides.</p>"},{"location":"guides/","title":"User Guides","text":""},{"location":"guides/#setup","title":"Setup","text":"<ul> <li>DWS on CSM</li> </ul>"},{"location":"guides/dws-on-csm/readme/","title":"DWS On CSM: Installation and use of DWS on a CSM cluster","text":""},{"location":"guides/dws-on-csm/readme/#install-dws","title":"Install DWS","text":""},{"location":"guides/dws-on-csm/readme/#dws-dependencies","title":"DWS Dependencies","text":"<p>Install the DWS dependencies and set the expected node labels prior to installing DWS.</p> DWS Dependencies     DWS requires kube-rbac-proxy to be present in the cluster's container registry.  ncn-m001:~ #<pre><code>podman run --rm --network host quay.io/skopeo/stable:v1.4.1 copy --dest-tls-verify=false docker://gcr.io/kubebuilder/kube-rbac-proxy:v0.13.0 docker://registry.local/kube-rbac-proxy:v0.13.0\n</code></pre>  DWS will be deployed on a kubernetes worker node labeled with `cray.wlm.manager`.  ncn-m001:~ #<pre><code>kubectl label node ncn-w001 cray.wlm.manager=true\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#retrieve-dws-configuration-and-container-image","title":"Retrieve DWS Configuration and Container Image","text":"<p>The DWS Configuration is in the DWS repo.  We don't need to build DWS here, but we do need its configuration files from the repo.  This is where we will get the DWS CRDs, Deployment, ServiceAccount, Roles and bindings, Services, and Secrets.</p> ncn-m001:~ #<pre><code>DWS_VER=0.0.6\ngit clone --branch v$DWS_VER https://github.com/HewlettPackard/dws.git\ncd dws\n</code></pre> <p>Get the DWS container image corresponding to this repo.  This must be made present in the cluster's container registry.</p> ncn-m001:~/dws #<pre><code>podman run --rm --network host quay.io/skopeo/stable:v1.4.1 copy --dest-tls-verify=false docker://ghcr.io/hewlettpackard/dws:$DWS_VER docker://registry.local/dws:$DWS_VER\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#deploy-dws-to-the-cluster","title":"Deploy DWS to the cluster","text":"<p>Deploy DWS to the cluster.  Set <code>IMAGE_TAG_BASE</code> to point at the DWS image in the cluster's container registry from the previous step.  Set the <code>OVERLAY=csm</code> to pick the configuration for CSM clusters.</p> ncn-m001:~/dws #<pre><code>IMAGE_TAG_BASE=registry.local/dws OVERLAY=csm make deploy\n</code></pre> <p>Wait for the deployment to become ready.</p> ncn-m001:~/dws #<pre><code>kubectl wait deployment --timeout=60s -n dws-operator-system --for condition=Available=True dws-operator-controller-manager\n</code></pre> <p>To undeploy DWS, after all Workflow resources have been deleted:</p> ncn-m001:~/dws #<pre><code>IMAGE_TAG_BASE=registry.local/dws OVERLAY=csm make undeploy\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#configure-slurm-for-dws","title":"Configure Slurm for DWS","text":"<p>Slurm provides an API for burst buffer plugins that it uses to talk to a workload manager (WLM).  The plugin is written in the Lua programming language and is placed in the same directory that holds the rest of the Slurm configuration files.  This plugin contains the logic required to communicate with DWS.</p> <p>This project will install two files into the slurm pod's <code>/etc/slurm</code> directory and will update the main Slurm configuration file to enable the plugin.  The first will be the Lua plugin script, named <code>burst_buffer.lua</code>, and the second will be a configuration file named <code>burst_buffer.conf</code> which tells Slurm that job scripts containing a <code>#DW</code> directive should be run through the burst_buffer plugin.</p>"},{"location":"guides/dws-on-csm/readme/#hpe-cray-programming-environment-installation-guide","title":"HPE Cray Programming Environment Installation Guide","text":"<p>The following instructions use section 10.3.2 in HPE Cray Programming Environment Installation Guide: CSM on HPE Cray EX Systems (22.10) S-8003.</p>"},{"location":"guides/dws-on-csm/readme/#checkout-the-dws-repo-for-slurm-plugins","title":"Checkout the DWS repo for Slurm plugins.","text":"ncn-m001:~ #<pre><code>git clone https://github.com/DataWorkflowServices/dws-slurm-bb-plugin.git\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#update-the-slurm-configuration-template","title":"Update the Slurm Configuration Template","text":"<p>The changes to the Slurm pod's <code>/etc/slurm</code> directory are controlled through a ConfigMap.  These instructions follow section 10.3.2 in HPE Cray Programming Environment Installation Guide: CSM on HPE Cray EX Systems (22.10) S-8003 to update and activate the ConfigMap.</p> <p>Get the slurm-config-templates ConfigMap.</p> ncn-m001:~ #<pre><code>kubectl get configmap -n services slurm-config-templates -o yaml &gt; slurm-config-templates.yaml\n</code></pre> <p>Extract the <code>slurm.conf</code> file, update it to enable the burst buffer plugin, and write it back into the ConfigMap.</p> ncn-m001:~ #<pre><code>yq r slurm-config-templates.yaml 'data.\"slurm.conf\"' &gt; slurm.conf\necho \"BurstBufferType=burst_buffer/lua\" &gt;&gt; slurm.conf\nyq w -i slurm-config-templates.yaml 'data.\"slurm.conf\"' \"$(cat slurm.conf)\"\n</code></pre> <p>Add the <code>burst_buffer.lua</code> plugin script and <code>burst_buffer.conf</code> configuration file to the ConfigMap.</p> ncn-m001:~ #<pre><code>yq w -i slurm-config-templates.yaml -- 'data.\"burst_buffer.lua\"' \"$(cat dws-slurm-bb-plugin/src/burst_buffer/burst_buffer.lua)\"\nyq w -i slurm-config-templates.yaml 'data.\"burst_buffer.conf\"' \"$(cat dws-slurm-bb-plugin/src/burst_buffer/burst_buffer.conf)\"\n</code></pre> <p>Apply the updated ConfigMap resource.</p> ncn-m001:~ #<pre><code>kubectl apply -f slurm-config-templates.yaml\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#restart-the-slurm-configuration-job","title":"Restart the Slurm Configuration Job","text":"<p>The Slurm configuration job will process the ConfigMap into a second one that is included in the pod spec.  The following steps to reconfigure Slurm are found in the HPE Cray Programming Installation Guide.</p> ncn-m001:~ #<pre><code>JOBNAME=$(kubectl get job -n services | grep slurm-config | grep -v import | awk '{print $1}')\nkubectl get job -n services -o yaml $JOBNAME &gt; slurm-config.yaml\nkubectl delete -f slurm-config.yaml\nyq d -i slurm-config.yaml spec.template.metadata\nyq d -i slurm-config.yaml spec.selector\nkubectl apply -f slurm-config.yaml\nSLURMCTLD_POD=$(kubectl get pod -n user -lapp=slurmctld -o jsonpath='{.items[0].metadata.name}')\nkubectl exec -n user $SLURMCTLD_POD -c slurmctld -- scontrol reconfigure\n</code></pre> <p>After the configuration job has been restarted, wait about 30 seconds to see the new content appear inside the pod.</p> <p>If you were only adding or modifying burst_buffer.lua, then Slurm is now ready for your jobs.  If you were also modifying slurm.conf to set the BurstBufferType, or you were adding or modifying burst_buffer.conf, then you must also restart the Slurm deployments.</p> ncn-m001:~ #<pre><code>kubectl rollout restart deployment -n user slurmctld-backup\nkubectl wait deployment --timeout=60s -n user --for condition=Available=True -l app=slurmctld-backup\nkubectl rollout restart deployment -n user slurmctld\nkubectl wait deployment --timeout=60s -n user --for condition=Available=True -l app=slurmctld\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#submit-a-test-job","title":"Submit a test job","text":"<p>Use a UAN host to submit a test job.  The following assumes a UAN host named <code>uan01</code> and a user account named <code>vers</code>.</p> <pre><code>ncn-m001:~ # ssh uan01\nuan01:~ # su vers\nvers@uan01:/root&gt; cd /lus/vers\n</code></pre> <p>Create the following example test job:</p> vers@uan01:/lus/vers&gt; cat /tmp/dws-test<pre><code>#!/bin/sh                \n#SBATCH --time=1\n#DW\n/bin/hostname\nsrun -l /bin/hostname\nsrun -l /bin/pwd\n</code></pre> <p>Note that we will submit the test job from a directory where our <code>vers</code> account can write its output files.  The output file location can be controlled by the <code>sbatch</code> command or by an <code>SBATCH</code> directive in the job script.</p> vers@uan01:/lus/vers&gt;<pre><code>sbatch /tmp/dws-test\n</code></pre> <p>The sbatch command will tell you the ID of your job. <pre><code>Submitted batch job 7773\n</code></pre></p> <p>You can use that to query the job's status.  In this example the job's output will be in the file named <code>slurm-&lt;ID&gt;</code> in the same directory where the <code>sbatch</code> command was executed.</p> vers@uan01:/lus/vers&gt;<pre><code>sacct -b -j 7773\nscontrol show job 7773\ncat slurm-7773.out\n</code></pre> <p>Get the status of the workflow.</p> vers@uan01:/lus/vers&gt;<pre><code>scontrol show bbstat workflow 7773 &amp;&amp; echo\n</code></pre> <p>The output will appear as:</p> <pre><code>desiredState=DataIn currentState=DataIn status=Completed\n</code></pre> <p>Note that the workflow resource will no longer exist after Slurm has completed its teardown state for this job.</p>"},{"location":"guides/dws-on-csm/readme/#canceling-a-test-job","title":"Canceling a test job","text":"<p>Use of the <code>scancel</code> command in states prior to PostRun will cause Slurm to pass the <code>hurry</code> flag to the teardown function in the burst_buffer.lua script, and the teardown function will set the flag in the Workflow resource.  In the PostRun or DataOut states the burst_buffer.lua teardown function will not be passed the <code>hurry</code> flag because no work will be skipped.  In all states, the <code>scancel --hurry</code> command will cause Slurm to pass the <code>hurry</code> flag to the teardown function.</p> <p>The <code>scancel</code> command will cause states prior to PostRun to terminate immediately and proceed to Teardown state.  The use of <code>scancel --hurry</code> does not alter this behavior on these pre-PostRun states.  During PostRun or DataOut, the <code>scancel</code> command does not cause early termination and does not skip the DataOut state.  The use of <code>scancel --hurry</code> during PostRun or DataOut causes early termination, skipping DataOut in the case of PostRun, and proceeds to Teardown.</p> <p>Consult the Slurm Burst Buffer Guide for further details on the use of <code>scancel</code> versus <code>scancel --hurry</code>.</p>"},{"location":"guides/dws-on-csm/readme/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/dws-on-csm/readme/#collect-slurmctld-logs","title":"Collect slurmctld logs","text":"<p>To collect the slurmctld logs:</p> ncn-m001:~ #<pre><code>SLURM_POD=`kubectl get pods -n user -l app=slurmctld -o jsonpath='{.items[0].metadata.name}'`\nkubectl logs -n user $SLURM_POD slurmctld\n</code></pre> <p>To pick out the messages from the burst_buffer.lua script:</p> ncn-m001:~ #<pre><code>kubectl logs -n user $SLURM_POD slurmctld | grep lua:\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#collect-dws-logs","title":"Collect DWS logs","text":"<p>The DWS logs can be retrieved with the following:</p> ncn-m001:~ #<pre><code>DWS_POD=`kubectl get pods -n dws-operator-system -l control-plane=controller-manager -o jsonpath='{.items[0].metadata.name}'`\nkubectl logs -n dws-operator-system $DWS_POD -c manager\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#inspect-dws-workflow-resources","title":"Inspect DWS Workflow resources","text":"<p>Inspect DWS Workflow resources by using the fully-qualified CRD name.</p> ncn-m001:~ #<pre><code>kubectl get workflows.dws.cray.hpe.com -A\n</code></pre> <p>The output will show the job ID used in the name of the Workflow resource:</p> <pre><code>NAMESPACE   NAME     STATE    READY   STATUS      AGE\nuser        bb7773   DataIn   true    Completed   13m\n</code></pre> <p>To get more detail about the workflow while it exists:</p> ncn-m001:~ #<pre><code>kubectl get workflow.dws.cray.hpe.com -n user bb7773 -o yaml\n</code></pre> <p>Note that the Workflow resource will be deleted during the job's teardown state, at which time it will no longer appear in the workflow listing.</p> Multiple CRDs named \"Workflow\"     The fully-qualified name will protect you from picking up any resource named \"Workflow\" that belongs to some other service.  Your system may have multiple CRDs named \"Workflow\".  To see the CRDs named \"Workflow\":  ncn-m001:~ #<pre><code>kubectl get crds | grep -E '^workflows\\.'\n</code></pre>  Example output:  <pre><code>workflows.argoproj.io                            2022-11-25T02:56:48Z\nworkflows.dws.cray.hpe.com                       2022-11-21T15:38:13Z\n</code></pre>"},{"location":"guides/dws-on-csm/readme/#references","title":"References","text":"<ul> <li>Section 10.3.2 of HPE Cray Programming Environment Installation Guide: CSM on HPE Cray EX Systems (22.10) S-8003</li> <li>Slurm Burst Buffer Guide </li> <li>Slurm burst_buffer.conf manage</li> </ul>"},{"location":"repo-guides/","title":"Repo Guides","text":""},{"location":"repo-guides/#management","title":"Management","text":"<ul> <li>Create a Release</li> </ul>"},{"location":"repo-guides/create-a-release/readme/","title":"Create a Release: Notes about creating a release","text":"<p>The following notes describe how to use the GitHub release process to create a release of a repo, and the workflows in place to guide this process.  This includes the use of release branches and annotated release tags.</p>"},{"location":"repo-guides/create-a-release/readme/#github-release-process","title":"GitHub Release Process","text":"<p>See About releases for the GitHub documentation on releases.</p> <p>A release can be created on GitHub by clicking the <code>Create a new release</code> button in the repo.  Select an annotated tag for your release. Do not use this interface to create a tag because that tag will not be annotated and it will fail the repo's workflow steps.  See the steps below to create an annotated tag, then proceed to the remaining steps in the GitHub documentation.</p>"},{"location":"repo-guides/create-a-release/readme/#release-branches","title":"Release branches","text":"<p>Release branches are named <code>releases/vX</code>, where X is 1, 2, etc.  In the usual case, all version 1.x.y release tags will go into the <code>releases/v1</code> branch, and all version 2.x.y tags will go into the <code>releases/v2</code> branch.  Fine-grained branches such as <code>releases/v2.1</code> for all version 2.1.y tags may be used if necessary.</p> <p>The repo has a \"branch protection rule\" on the branch pattern of <code>releases/v*</code> to enforce the use of pull requests and reviews on release branches.  See Managing a branch protection rule.</p>"},{"location":"repo-guides/create-a-release/readme/#release-tags","title":"Release tags","text":"<p>Release tags are named in patterns like <code>vX.Y.Z</code> or <code>vX.Y.Z-alpha</code> and are applied to the release branches.</p> <p>The repo has a \"tag protection rule\" on the tag pattern of <code>v*</code>.  Only users who have admin or maintain permission may create or delete protected tags.  See Configuring tag protection rules.</p> <p>IMPORTANT The tag must be applied to a commit that is entirely within the release branch, so that it is not visible from the main branch.  If it is visible on the main branch, then image tags from the main branch will collide with image tags on the release branch.  This means that after you create the release branch you may have to add another commit to it simply to have a place to attach the tag.</p> Verify tag visibility     After you've created the tag in your workarea, but before you've pushed it to the repo, verify that it is visible only on the release branch and not on the main branch.  Look for tags on the main branch<pre><code>git checkout main\ngit describe --match=\"v*\" HEAD\n</code></pre> Look for tags on the release branch<pre><code>git checkout releases/v1\ngit describe --match=\"v*\" HEAD\n</code></pre>"},{"location":"repo-guides/create-a-release/readme/#annotated-tags","title":"Annotated tags","text":"<p>Our process requires that release tags be annotated.  An annotated tag contains information about who created it and when it was created.  Annotated tags may also be signed.</p> <p>The repo contains a workflow which verifies that any pushed tag is an annotated tag.</p>"},{"location":"repo-guides/create-a-release/readme/#creating-an-annotated-tag","title":"Creating an annotated tag.","text":"<p>You can use GitHub Desktop to create an annotated tag, or you can create the tag with the <code>git tag -a</code> command.</p> Example annotated tag<pre><code>git checkout releases/v1\ngit tag -a v1.0.2 -m \"My release 1.0.2\"\ngit push origin --tags\n</code></pre> Verify that a tag is annotated When a tag is annotated, its details will be displayed before the commit the tag references.  Show a tag<pre><code>git show v0.0.6\n</code></pre>  If the tag is annotated then its details will be shown before the commit is shown.   An annotated tag<pre><code>tag v0.0.6\nTagger: Dean Roehrich &lt;dean.roehrich@hpe.com&gt;\nDate:   Wed Dec 14 15:15:46 2022 -0600\n\nExperimental release #8\n\ncommit 91fea7d02a5e7196ca8b4686deb33898109fd8c0 (HEAD, tag: v0.0.6)\nAuthor: Dean Roehrich &lt;dean.roehrich@hpe.com&gt;\nDate:   Wed Dec 14 14:44:15 2022 -0600\n\n    Rename the workflow that verifies tags. (#82)\n\n    Signed-off-by: Dean Roehrich &lt;dean.roehrich@hpe.com&gt;\n\n[...]\n</code></pre>"}]}